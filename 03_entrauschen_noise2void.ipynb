{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrauschen mit Noise2Void\n",
    "Dieses Notebook ist dem 2D-Beispiel [denoising2D_SEM](https://github.com/juglab/n2v/tree/master/examples/2D/denoising2D_SEM) der GitHub Implementierung entnommen.\n",
    "\n",
    "Nun wenden wir uns dem entrauschen mit DeepLearning zu. Das Entrauschen war bei den *E. Coli* Swarming Daten nicht das Problem. Bei vielen unserer Mikroskopbildern für Biofilme ist das Entrauschen allerdings komplizierter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all our dependencies.\n",
    "import tensorflow as tf\n",
    "from n2v.models import N2VConfig, N2V\n",
    "import skimage.filters\n",
    "import numpy as np\n",
    "from csbdeep.utils import plot_history\n",
    "from n2v.utils.n2v_utils import manipulate_val_data\n",
    "from n2v.internals.N2V_DataGenerator import N2V_DataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import urllib\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswahl einer GPU\n",
    "Auf dieser Workstation stehen 4 GPUs zur Verfügung. Damit in diesem Notebook nicht alle vier GPUs allokiert werden, sagen wir CUDA, dass nur die erste GPU genutzt werden soll. Dazu setzen wir die [Umgebungsvariable](https://de.wikipedia.org/wiki/Umgebungsvariable) `CUDA_VISIBLE_DEVICES`. Mit tf.config.list_physical_devices können wir uns die zur Verfügung stehenden Recheneinheiten anzeigen lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we load __one__ set of low-SNR images and use the <code>N2V_DataGenerator</code> to extract training <code>X</code> and validation <code>X_val</code> patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our DataGenerator-object.\n",
    "# It will help us load data and extract patches for training and validation.\n",
    "datagen = N2V_DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load all the '.tif' files from the 'data' directory.\n",
    "# If you want to load other types of files see the RGB example.\n",
    "# The function will return a list of images (numpy arrays).\n",
    "#imgs = datagen.load_imgs_from_directory(directory = \"data/\")\n",
    "imgs = datagen.load_imgs_from_directory(directory = \"/extdata/readonly/f-prak-v15/v-cholera-biofilm\")\n",
    "\n",
    "# Let's look at the shape of the images.\n",
    "print(imgs[0].shape, imgs[1].shape)\n",
    "# The function automatically added two extra dimensions to the images:\n",
    "# One at the beginning, is used to hold a potential stack of images such as a movie.\n",
    "# One at the end, represents channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrauschen mit Gauss\n",
    "Wir schauen uns ein zufälliges Bild an und entrauschen es mit einem Gaußfilter. Ihr könnt auch andere Filter nutzen, um das Bild zu entrauschen und andere Sizes und Sigmas verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imgs[np.random.randint(0, len(imgs))][0, 300:700, 300:700, 0]\n",
    "denoise = skimage.filters.gaussian(img.astype(float), 0.8)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Raw image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(denoise)\n",
    "plt.axis('off')\n",
    "plt.title('Gauß filter')\n",
    "None;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrauschen mit Deep Learning\n",
    "Noise2Void ist eine unsupervised Methode. Deep Learning Methoden müssen stets auf gewissen Daten trainiert werden. Im folgenden werden die Bilder, die wir oben eingelesen haben, für das Training vorbereitet. Dabei werden aus den großen Bildern Patches ausgeschnitten. Zudem werden wieder Trainings- und Validierungsset erstellt. Ein Test-Set entfällt hier, da die Methode Unsupervised ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the first image to extract training patches and store them in 'X'\n",
    "imgs_len = len(imgs)\n",
    "X = datagen.generate_patches_from_list(imgs[:imgs_len//4*3], shape=(96,96))\n",
    "\n",
    "# We will use the second image to extract validation patches.\n",
    "X_val = datagen.generate_patches_from_list(imgs[imgs_len//4:], shape=(96,96))\n",
    "\n",
    "# Patches are created so they do not overlap.\n",
    "# (Note: this is not the case if you specify a number of patches. See the docstring for details!)\n",
    "# Non-overlapping patches would also allow us to split them into a training and validation set \n",
    "# per image. This might be an interesting alternative to the split we performed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case you don't know how to access the docstring of a method:\n",
    "#datagen.generate_patches_from_list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at one of our training and validation patches.\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X[0,...,0], cmap='magma')\n",
    "plt.title('Training Patch');\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(X_val[100,...,0], cmap='magma')\n",
    "plt.title('Validation Patch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfiguration\n",
    "Deep Learning Modelle suchen zwar selbst eigene Optimale Parameter, um gute Resultate zu erzielen. Trotzdem können sehr viele Konfigurationen vorgenommen werden. Diese Konfigurationen betreffen die Architektur des Netzwerks, die Art und Länge des Trainings und vieles mehr. Viele Methoden können mit einer Config-Klasse konfiguriert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise2Void comes with a special config-object, where we store network-architecture and training specific parameters. See the docstring of the <code>N2VConfig</code> constructor for a description of all parameters.\n",
    "\n",
    "When creating the config-object, we provide the training data <code>X</code>. From <code>X</code> we extract <code>mean</code> and <code>std</code> that will be used to normalize all data before it is processed by the network. We also extract the dimensionality and number of channels from <code>X</code>.\n",
    "\n",
    "Compared to supervised training (i.e. traditional CARE), we recommend to use N2V with an increased <code>train_batch_size</code> and <code>batch_norm</code>.\n",
    "To keep the network from learning the identity we have to manipulate the input pixels during training. For this we have the parameter <code>n2v_manipulator</code> with default value <code>'uniform_withCP'</code>. Most pixel manipulators will compute the replacement value based on a neighborhood. With <code>n2v_neighborhood_radius</code> we can control its size. \n",
    "\n",
    "Other pixel manipulators:\n",
    "* normal_withoutCP: samples the neighborhood according to a normal gaussian distribution, but without the center pixel\n",
    "* normal_additive: adds a random number to the original pixel value. The random number is sampled from a gaussian distribution with zero-mean and sigma = <code>n2v_neighborhood_radius</code>\n",
    "* normal_fitted: uses a random value from a gaussian normal distribution with mean equal to the mean of the neighborhood and standard deviation equal to the standard deviation of the neighborhood.\n",
    "* identity: performs no pixel manipulation\n",
    "\n",
    "For faster training multiple pixels per input patch can be manipulated. In our experiments we manipulated about 0.198% of the input pixels per patch. For a patch size of 64 by 64 pixels this corresponds to about 8 pixels. This fraction can be tuned via <code>n2v_perc_pix</code>.\n",
    "\n",
    "For Noise2Void training it is possible to pass arbitrarily large patches to the training method. From these patches random subpatches of size <code>n2v_patch_shape</code> are extracted during training. Default patch shape is set to (64, 64).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Warning:</font> to make this example notebook execute faster, we have set <code>train_epochs</code> to only 10. <br>For better results we suggest 100 to 200 <code>train_epochs</code>.\n",
    "\n",
    "## Wir starten erstmal nur mit 10 Epochen\n",
    "Wenn wir den Rest des Notebooks verstanden haben, können wir länger trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_steps_per_epoch is set to (number of training patches)/(batch size), like this each training patch \n",
    "# is shown once per epoch. \n",
    "config = N2VConfig(X, unet_kern_size=3, \n",
    "                   train_steps_per_epoch=int(X.shape[0]/128), train_epochs=10, train_loss='mse', batch_norm=True, \n",
    "                   train_batch_size=128, n2v_perc_pix=0.198, n2v_patch_shape=(96, 96), \n",
    "                   n2v_manipulator='uniform_withCP', n2v_neighborhood_radius=5)\n",
    "\n",
    "# Let's look at the parameters stored in the config-object.\n",
    "vars(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a name used to identify the model\n",
    "model_name = 'n2v_biofilm'\n",
    "# the base directory in which our model will live\n",
    "basedir = 'models'\n",
    "# We are now creating our network model.\n",
    "model = N2V(config, model_name, basedir=basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Training the model will likely take some time. We recommend to monitor the progress with TensorBoard, which allows you to inspect the losses during training. Furthermore, you can look at the predictions for some of the validation images, which can be helpful to recognize problems early on.\n",
    "\n",
    "You can start TensorBoard in a terminal from the current working directory with tensorboard --logdir=. Then connect to http://localhost:6006/ with your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We are ready to start training now.\n",
    "history = model.train(X, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training, lets plot training and validation loss.\n",
    "Was kann man in diesen Graphen erkennen?\n",
    "\n",
    "## Richtiges Training\n",
    "Nachdem die ersten 10 Epochen durchgerechnet wurden, können wir die Epochenzahl erhöhen und das richtige Training laufen lassen. Dann können wir schonmal zum nächsten Notebook springen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(history.history.keys())))\n",
    "plt.figure(figsize=(16,5))\n",
    "plot_history(history,['loss','val_loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_val = imgs[0][0, ..., 0]\n",
    "pred_val = model.predict(input_val, axes='YX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "gauss = skimage.filters.gaussian(input_val, 1)\n",
    "slx = slice(400, 600)\n",
    "sly = slice(400, 600)\n",
    "# Let's look at the results.\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(input_val[sly, slx], cmap=\"magma\")\n",
    "plt.title('Input');\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(pred_val[sly, slx], cmap=\"magma\")\n",
    "plt.title('Prediction');\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(gauss[sly, slx], cmap=\"magma\")\n",
    "plt.title('Gauß Filter Sigma = {:0.1f}'.format(sigma));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
