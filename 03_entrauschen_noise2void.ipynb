{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise2Void - 2D Example for SEM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all our dependencies.\n",
    "from n2v.models import N2VConfig, N2V\n",
    "import numpy as np\n",
    "from csbdeep.utils import plot_history\n",
    "from n2v.utils.n2v_utils import manipulate_val_data\n",
    "from n2v.internals.N2V_DataGenerator import N2V_DataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import urllib\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Example Data\n",
    "Data by Reza Shahidi and Gaspar Jekely, Living Systems Institute, Exeter<br>\n",
    "Thanks!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for our data.\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "\n",
    "# check if data has been downloaded already\n",
    "zipPath=\"data/SEM.zip\"\n",
    "if not os.path.exists(zipPath):\n",
    "    #download and unzip data\n",
    "    data = urllib.request.urlretrieve('https://cloud.mpi-cbg.de/index.php/s/pXgfbobntrw06lC/download', zipPath)\n",
    "    with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we load __one__ set of low-SNR images and use the <code>N2V_DataGenerator</code> to extract training <code>X</code> and validation <code>X_val</code> patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our DataGenerator-object.\n",
    "# It will help us load data and extract patches for training and validation.\n",
    "datagen = N2V_DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load all the '.tif' files from the 'data' directory.\n",
    "# If you want to load other types of files see the RGB example.\n",
    "# The function will return a list of images (numpy arrays).\n",
    "#imgs = datagen.load_imgs_from_directory(directory = \"data/\")\n",
    "imgs = datagen.load_imgs_from_directory(directory = \"/extdata/readonly/f-prak-v15/input\")\n",
    "\n",
    "# Let's look at the shape of the images.\n",
    "print(imgs[0].shape,imgs[1].shape)\n",
    "# The function automatically added two extra dimensions to the images:\n",
    "# One at the beginning, is used to hold a potential stack of images such as a movie.\n",
    "# One at the end, represents channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets' look at the images.\n",
    "# We have to remove the added extra dimensions to display them as 2D images.\n",
    "plt.imshow(imgs[0][0,...,0], cmap='magma')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imgs[1][0,...,0], cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the first image to extract training patches and store them in 'X'\n",
    "imgs_len = len(imgs)\n",
    "X = datagen.generate_patches_from_list(imgs[:imgs_len//2], shape=(96,96))\n",
    "\n",
    "# We will use the second image to extract validation patches.\n",
    "X_val = datagen.generate_patches_from_list(imgs[imgs_len//2:], shape=(96,96))\n",
    "\n",
    "# Patches are created so they do not overlap.\n",
    "# (Note: this is not the case if you specify a number of patches. See the docstring for details!)\n",
    "# Non-overlapping patches would also allow us to split them into a training and validation set \n",
    "# per image. This might be an interesting alternative to the split we performed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case you don't know how to access the docstring of a method:\n",
    "#datagen.generate_patches_from_list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at one of our training and validation patches.\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X[0,...,0], cmap='magma')\n",
    "plt.title('Training Patch');\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(X_val[100,...,0], cmap='magma')\n",
    "plt.title('Validation Patch');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise2Void comes with a special config-object, where we store network-architecture and training specific parameters. See the docstring of the <code>N2VConfig</code> constructor for a description of all parameters.\n",
    "\n",
    "When creating the config-object, we provide the training data <code>X</code>. From <code>X</code> we extract <code>mean</code> and <code>std</code> that will be used to normalize all data before it is processed by the network. We also extract the dimensionality and number of channels from <code>X</code>.\n",
    "\n",
    "Compared to supervised training (i.e. traditional CARE), we recommend to use N2V with an increased <code>train_batch_size</code> and <code>batch_norm</code>.\n",
    "To keep the network from learning the identity we have to manipulate the input pixels during training. For this we have the parameter <code>n2v_manipulator</code> with default value <code>'uniform_withCP'</code>. Most pixel manipulators will compute the replacement value based on a neighborhood. With <code>n2v_neighborhood_radius</code> we can control its size. \n",
    "\n",
    "Other pixel manipulators:\n",
    "* normal_withoutCP: samples the neighborhood according to a normal gaussian distribution, but without the center pixel\n",
    "* normal_additive: adds a random number to the original pixel value. The random number is sampled from a gaussian distribution with zero-mean and sigma = <code>n2v_neighborhood_radius</code>\n",
    "* normal_fitted: uses a random value from a gaussian normal distribution with mean equal to the mean of the neighborhood and standard deviation equal to the standard deviation of the neighborhood.\n",
    "* identity: performs no pixel manipulation\n",
    "\n",
    "For faster training multiple pixels per input patch can be manipulated. In our experiments we manipulated about 0.198% of the input pixels per patch. For a patch size of 64 by 64 pixels this corresponds to about 8 pixels. This fraction can be tuned via <code>n2v_perc_pix</code>.\n",
    "\n",
    "For Noise2Void training it is possible to pass arbitrarily large patches to the training method. From these patches random subpatches of size <code>n2v_patch_shape</code> are extracted during training. Default patch shape is set to (64, 64).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Warning:</font> to make this example notebook execute faster, we have set <code>train_epochs</code> to only 10. <br>For better results we suggest 100 to 200 <code>train_epochs</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_steps_per_epoch is set to (number of training patches)/(batch size), like this each training patch \n",
    "# is shown once per epoch. \n",
    "config = N2VConfig(X, unet_kern_size=3, \n",
    "                   train_steps_per_epoch=int(X.shape[0]/128), train_epochs=10, train_loss='mse', batch_norm=True, \n",
    "                   train_batch_size=128, n2v_perc_pix=0.198, n2v_patch_shape=(96, 96), \n",
    "                   n2v_manipulator='uniform_withCP', n2v_neighborhood_radius=5)\n",
    "\n",
    "# Let's look at the parameters stored in the config-object.\n",
    "vars(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a name used to identify the model\n",
    "model_name = 'n2v_2D'\n",
    "# the base directory in which our model will live\n",
    "basedir = 'models'\n",
    "# We are now creating our network model.\n",
    "model = N2V(config, model_name, basedir=basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Training the model will likely take some time. We recommend to monitor the progress with TensorBoard, which allows you to inspect the losses during training. Furthermore, you can look at the predictions for some of the validation images, which can be helpful to recognize problems early on.\n",
    "\n",
    "You can start TensorBoard in a terminal from the current working directory with tensorboard --logdir=. Then connect to http://localhost:6006/ with your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We are ready to start training now.\n",
    "history = model.train(X, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training, lets plot training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(history.history.keys())))\n",
    "plt.figure(figsize=(16,5))\n",
    "plot_history(history,['loss','val_loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_val = imgs[imgs_len//2+1][0, ..., 0]\n",
    "pred_val = model.predict(input_val, axes='YX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slx = slice(100, 300)\n",
    "sly = slice(700, 1000)\n",
    "# Let's look at the results.\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(input_val[sly, slx], cmap=\"magma\")\n",
    "plt.title('Input');\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(pred_val[sly, slx], cmap=\"magma\")\n",
    "plt.title('Prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model to be Used with CSBDeep Fiji Plugins and KNIME Workflows\n",
    "See https://github.com/CSBDeep/CSBDeep_website/wiki/Your-Model-in-Fiji for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export_TF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
